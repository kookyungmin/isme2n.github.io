---
layout: post
title:  "[Tensorflow]9.Softmax classifier(2)"
date:   2017-08-05 19:00:00 -0500
comments: true
categories: tensorflow
---

## Softmax classifier(2)
<br>
[8.Softmax classifier(1)](https://kookyungmin.github.io/tensorflow/2017/08/05/tensorflow09.html)에 이어서 포스팅을 하겠습니다.
<br>
이번 장에서는 tensorflow로 처음부터 끝까지 간단하게 데이터를 훈련시키고 예측까지 시켜보겠습니다.
<br>
<br>
어떤 대상의 속성1 속성2 속성3 속성4를 입력하면 A,B,C 중 하나로 분류되게끔 학습을 시켜볼 것입니다~~
<br>
<br>
먼저, 맨 처음 tensorflow를 import~~!
<br>
<br>

>```python
>import tensorflow as tf
>```

<br>
<br>
그리고 학습 데이터를 선언해줍니다.
<br>
<br>

>```python
>// [속성1,속성2,속성3,속성4]
>x_data=[[1,2,1,1],[4,1,5,5],[2,1,3,2],[1,7,7,7]]
>y_data=[[0,0,1],[0,1,0],[0,0,1],[1,0,0]]  //[A,B,C]
>```

<br>
<br>
그 다음 학습에 필요한 변수들 선언!
<br>
<br>

>```python
>X=tf.placeholder(tf.float32,shape=[None,4]) 
>Y=tf.placeholder(tf.float32,shape=[None,3])
>//W의 shape는 X의 shape와 행렬곱해서 Y의 shape가 되어야하므로 [4,3]
>W=tf.Variable(tf.random_normal([4,3]),name='weight')
>b=tf.Variable(tf.random_normal([3]),name='bias')
>```

<br>
<br>
hypothesis 정의
<br>
<br>

>```python
>//softmax로 다시 정의된 hypothesis
>logits=tf.matmul(X,W)+b
>hypothesis=tf.nn.softmax(logits)
>```

<br>
<br>
cost 함수 정의
<br>
<br>

>```python
>//새로 정의된 cost 함수 (예측 값과 실제 값의 차이)
>cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y)
>cost=tf.reduce_mean(cost_i)
>```

<br>
<br>
cost 최소화
<br>
<br>

>```python
>//실제 값과 예측 값의 차이를 계속 줄여준다.
>optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0001)
>train=optimizer.minimize(cost)
>```

<br>
<br>
노드들을(tensor) 그래프화 
<br>
<br>

>```python
>sess=tf.Session()
>//변수 초기화
>sess.run(tf.global_variables_initializer())
>```

<br>
<br>
학습을 시작합니다~
<br>
<br>

>```python
>//반복문을 돌면서 계속 데이터를 넣는다->w,b가 cost가 최소가 되게 계속 이동
>for step in range(2001):
>	cost_val,_=sess.run([cost,train],feed_dict={X:x_data,Y:y_data})})
>	if(step%100==0):
>		print(step,cost_val)
>```

<br>
<br>
학습 끝! 이번에도 step마다 cost가 줄어드는 것을 보실 수 있습니다.
<br>
<br>

>![image](/image/tensorflow_img/s2.png)

<br>
<br>
과연 학습이 잘 되었나 확인해볼까요? [속성 1,속성 2,속성 3,속성 4]=[2,4,3,1] 일 때
<br>
<br>

>![image](/image/tensorflow_img/s3.png)

<br>
hypothesis의 예측 값이 모두 0과 1사이의 수로 나타난 것을 볼 수 있습니다!
<br>
<br>
그리고 argmax 함수를 써서 가장 큰 값을 갖는 인덱스를 구하면~
<br>

![image](/image/tensorflow_img/s4.png)

>tf.argmax 함수는 최댓값을 갖는 인덱스를 반환해준다.
><br>
>예를 들면 위의 result에서 첫번째 원소가 가장 크죠?? 그래서 인덱스 0을 반환한 것입니다. 만약 가운데가 가장 크면 1을 반환하겠죠~

<br>
<br>
결과가 0이 나왔으므로 위의 데이터는 A에 속한다고 예측할 수 있네요~
<br>
처음 y_data를 집어 넣을 때 A에 속한다면 [[1],[0],[0]]을 넣으신 것을 기억하시나요??
<br> 
따라서 인덱스는 다음과 같습니다.
<br>
A:0 B:1 C:2
<br>
<br>
이제 우리는 충분한 데이터만 있다면 데이터를 예측하는 프로그램을 어느정도 작성할 수 있게 되었습니다~!
<br>
<br>
지금 한 예제의 틀만 제대로 이해하신다면 해내실 수 있습니다!!! (디테일은 알아서.. 죄송합니다.)
<br>
<br>
과연 이 정도 배우고 할 수 있을지 의문이 드신다구요?? 
<br>
<br>
그럼 다음 장에서 그 대답을 확인해보시죠!!ㅋㅋㅋㅋ
<br>
<br>
다음 장에서 뵙겠습니다~!
<br>
<br>
전체 코드
<br>
<br>

>![image](/image/tensorflow_img/s1.png)

<br>
<br>
<br>
- - -
이전 장 [[Tensorflow]8.Softmax classifier(1)](https://kookyungmin.github.io/tensorflow/2017/08/05/tensorflow09.html)
<br>
다음 장 [[Tensorflow]10.MNIST 91%(1)](https://kookyungmin.github.io/tensorflow/2017/08/05/tensorflow11.html)
<br>
<br>
<br>
By 꾸리
