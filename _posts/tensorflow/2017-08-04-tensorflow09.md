---
layout: post
title:  "[Tensorflow]8.Softmax classifier(1)"
date:   2017-08-05 10:00:00 -0500
comments: true
categories: tensorflow
---


## softmax classifier(1)

<br>
전 장의 예제에서는 어느 학생의 국어,영어,수학,과학 점수를 가지고서 대학의 합격여부를 예측하게끔 학습을 하였습니다. 
<br>
<br>
이번시간에는 분류(classification) 기준이 2개가 아니라 분류기준이 다수인 Multinomial classfication를 배워 보겠습니다.
<br>
<br>
전 장의 예제에서는 단순히 합격(0), 불합격(1)으로 분류 기준이 2개였습니다.
<br>
multinomial classfication은 분류 기준이 더 많은 경우입니다.
<br>
<br>
예를 들면, 어떤 대상의 속성1,속성2,속성3,속성4를 입력하면 A,B,C로 분류 되도록 학습시킬 수가 있습니다. (분류 기준이 3개) 
<br>
<br>
Multinomial classfication도 4단계로 이뤄진다고 보면 됩니다.
<br>
1.training data를 준비한다.
<br>
2.Hypothesis(판별식)을 설정한다.
<br>
3.Cost함수를 설정한다.
<br>
4.cost함수를 최소화시키는 W,b를 구한다.
<br>
<br>

## 1.Training data set을 준비한다.
<br>

x_data(속성 값), y_data(결괏 값)을 따로 선언해줍니다.(실제 데이터)
<br>
<br>

>훈련 데이터 4개 이용하였습니다.(정확도를 높이려면 훨씬 많이 필요합니다.)
><br>
><br>
>
>```python
>// [속성1,속성2,속성3,속성4]
>x_data=[[1,2,1,1],[4,1,5,5],[2,1,3,2],[1,7,7,7]]
>y_data=[[0,0,1],[0,1,0],[0,0,1],[1,0,0]]  //[A,B,C]
>
>y_data는 분류 기준을 나타내기에 만약 어떤 대상이 A로 분류 되었다면,
>y_data는 [1,0,0] 으로 하나가 1이면 나머지 성분은 0 이어야합니다.
>```

<br>

## 2.Hypothesis(판별식)을 설정한다.

<br>

>전 장에서의 hypothesis는 sigmoid 함수를 이용하였는데요!
><br>
>분류 기준이 늘어남에 따라 y_data의 성분도 늘어나서 hypothesis의 성분도 더 이상 한 개가 아닙니다~
><br>
><br>
>hypothesis의 각 성분마다 sigmoid 함수를 적용하고 싶은데요!
><br>
>이를 해결해주는 것이 바로 softmax 함수입니다!
><br>
><br>
>자세한 설명은 생략할게요..
><br>
>그냥 앞으로는 분류(classification)를 할 때 분류기준이 1개이든지 다수이든지 간에 soft max 함수를 사용하면 된다고 생각하시면 됩니다.
><br>
soft max 함수를 이용하면 hypothesis의 성분 값 모두 0과 1사이의 수가 됩니다!
><br>
><br>
>그리고 그 중 가장 큰 값을 찾아서 분류하는 것입니다.

<br>

그럼 새로운 hypothesis를 정의합니다~
<br>
<br>

>```python
>logits=tf.matmul(X,W)+b
>hypothesis=tf.nn.softmax(logits)
>```

<br>
<br>

## 3.Cost함수를 설정한다.

<br>

>cost 함수는 우리가 hypothesis로 설정한 직선으로 구한 예측 값과 실제 값이 얼마나 다른가를 나타내는 함수이였습니다.
><br>
><br>
>hypothesis가 변함에 따라 cost함수도 변하게 됩니다~~~

<br>

새로운 cost함수는 다음과 같습니다.
<br>
<br>

>```python
>cost=tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))
>```
><br>
>되게 복잡합니다.. 하지만 걱정안하셔도 됩니다~ 이해하려고 노력 안 하셔도되요~!

<br>
더 간단히 정의할 수 있습니다. 다음과 같습니다~
<br>
<br>

>```python
>cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y)
>cost=tf.reduce_mean(cost_i)
>
>//logits=tf.matmul(X,W)+b입니다.
>```
>
><br>
>더 복잡해보이나요??ㅠㅠ 복잡해보이지만 자주 쓰다보면 수식이 필요없기에 더 편리합니다.

<br>
<br>

## 4.cost함수를 최소화시키는 W,b를 구한다.

<br>

>컴퓨터는 데이터를 훈련시키면서 cost 함수(우리가 예측한 값과 실제 값의 차이)를 작게 하는 W와 b 값을 구합니다.

<br>
<br>
이 부분은 역시 같습니다~!
<br>
<br>

>```python
>optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)
>train=optimizer.minimize(cost)
>```

<br>
<br>
다음 장에서 직접 tensorflow를 이용해서 직접 훈련도 시켜보고 예측 값도 구해보겠습니다.
<br>
<br>
<br>
- - -
이전 장 [[Tensorflow]7.Logistic(regression) classifier(2)](https://kookyungmin.github.io/tensorflow/2017/08/04/tensorflow08.html)
<br>
다음 장 [[Tensorflow]9.Softmax classifier(2)](https://kookyungmin.github.io/tensorflow/2017/08/05/tensorflow10.html)
<br>
<br>
<br>
By 꾸리
