---

layout: post

title:  "[Tensorflow]13.Neural Network"

subtitle: "[Tensorflow]13.Neural Network"

date:   2017-08-06 12:59:00 -0500

categories: Language

tags: tensorflow

---

## Neural Network

<br>
전 장에서 딥러닝에 대해 간단히 배워봤는데요~! 
<br>
<br>
이번 시간에는 Neural Network를 tensorflow에 적용하기 위해 필요한 것을 간단히 배워보겠습니다.
<br>
<br>
1. relu 함수 이용 방법
<br>
2. Xavier 초기화 값 이용 방법
<br>
3. Layer 구현 방법
<br>
4. Drop out
<br>
<br>

저번 시간에 Neural Network에 대한 문제점을 해결해 준 것이 relu함수와 Xavier 초기화 값이라고 한 걸 기억하시나요?? 
<br>
이를 tensorflow에서 어떻게 사용하는지 알아보고 layer를 구현해보겠습니다.

## 1. relu 함수 이용방법

<br>
<br>

relu함수는 다음과 같이 선언합니다.
<br>
<br>

```python
L=tf.nn.relu(tf.matmul(X,W)+b)
```

<br>
<br>
엄청 간단하죠??? softmax와 선언방법이 비슷합니다~~
<br>
<br>

## 2. Xavier 초기화 값 이용방법

<br>
<br>
지금까지는 Weight 변수를 초기화 할 때 랜덤 값을 주었는데, 
<br>
좋은 초기 값을 주어야 neural network로 학습을 잘 시킬 수 있습니다.
<br>
그 중 하나인 Xavier 초기 값을 주는 방법입니다~
<br>
<br>

```python
W=tf.get_variable("W",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())
```

<br>
<br>
엄청 복잡합니다.ㅠㅠ 위의 코드는 초기 값을 xavier로 주고, variable을 만들어줍니다.
<br>
<br>

## 3. Layer 구현방법

<br>
<br>
이제 이를 이용하여 input layer, hidden layer, output layer를 구현하는 방법을 알아보겠습니다.
<br>
<br>

```python
#input layer
W1=tf.get_variable("W1",shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())
b1=tf.Variable(tf.random_normal([512]))
L1=tf.nn.relu(tf.matmul(X,W1)+b1)

#hidden layer1
W2=tf.get_variable("W2",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())
b2=tf.Variable(tf.random_normal([512]))
L2=tf.nn.relu(tf.matmul(L1,W2)+b2)

#hidden layer 2
W3=tf.get_variable("W3",shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())
b3=tf.Variable(tf.random_normal([512]))
L3=tf.nn.relu(tf.matmul(L2,W3)+b3)

#output layer
W4=tf.get_variable("W4",shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())
b4=tf.Variable(tf.random_normal([10]))
hypothesis=tf.matmul(L3,W4)+b4
```

<br>
<br>
hidden을 구현할 때 relu함수와 xavier 초기 값을 이용하신 것을 보실 수 있습니다.
<br>
또, 층마다 반복되는 구조여서 그리 어렵지도 않습니다~~
<br>
<br>

## 4. Drop out

<br>
<br>
마지막으로 학습을 시킬 때 정확도를 높이는 방법 Drop out에 대해 알아볼까요??
<br>
Drop out은 간단히 설명하면 hidden layer에서 어떤 layer의 출력 값을 다음 layer의 입력 값으로 이용하는데,
<br>
출력 값 전부를 사용하지 않고 일부만을 사용하는 것입니다.
<br>
일부만 사용하여 학습시키면 특정한 데이터에 치우쳐서 학습되는 것을 방지할 수 있습니다~
<br>
<br>

```python
L2=tf.nn.dropout(L2,keep_prob=0.7)
#keep_prob는 어느 정도 사용할 것인가를 정해줍니다. 1이면 전부를 사용합니다.
```

<br>
<br>
이렇게 layer를 만드는 방법과 정확도를 높이는 방법까지 배워 봤습니다.
<br>
<br>
다음 장에서는 neural network를 사용하여 저번에 했던 MNIST를 다시 구현해보겠습니다~!

