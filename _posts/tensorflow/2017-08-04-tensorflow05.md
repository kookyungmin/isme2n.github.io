---

layout: post

title:  "[Tensorflow]4.Multi-variable linear regression(1)"

subtitle: "[Tensorflow]4.Multi-variable linear regression(1)"

date:   2017-08-04 10:59:00 -0500

categories: Language

tags: tensorflow

---



## Multi-variable linear regression(1)

<br>
전 장의 예제에서는 어느 학생의 수학점수를 가지고서 대학의 합격여부를 예측하게끔 학습을 하였습니다. 
<br>
하지만 대학을 수학점수만으로 가지 않죠~
<br>
국어점수, 영어점수, 과학점수 등 여러 과목의 점수가 필요합니다.
<br>
<br>
<br>
이번시간에는 데이터 하나(1명)의 속성이 하나(수학점수)가 아닌,
<br>
다수(국,영,수,과)일 때의 linear regression에 대해 배워 보겠습니다.
<br>
<br>
<br>
다수 일때도 머신러닝의 단계는 똑같습니다. 약간의 변화만 있을 뿐입니다.
<br>
잘 모르시는 분은 [3.Linear regression(1)](http://kookyungmin.github.io/language/2017/08/03/tensorflow03/) 을 참고하시길 바랍니다.
<br>
<br>
<br>

Multi-variable linear regression도 4단계로 이뤄진다고 보면 됩니다.
<br>
1.training data를 준비한다.
<br>
2.Hypothesis(판별식)을 설정한다.
<br>
3.Cost함수를 설정한다.
<br>
4.cost함수를 최소화시키는 W,b를 구한다.
<br>
<br>
<br>

## 1.Training data set을 준비한다.
<br>

>x_data(속성 값), y_data(결괏 값)을 따로 선언해줍니다.(실제 데이터)
><br>
><br>
>여기서 shape가 헷갈릴 수가 있는데, 행렬과 비슷합니다~ 많이 하다보시면 익숙해지실겁니다!
><br>
>기본적인 tensorflow 용어와 함수를 따로 정리해서 나중에 올려드리겠습니다.

<br>
training data를 다음과 같이 선언
<br>
<br>

>```python
># [학생1[국,영,수,과],학생2[국,영,수,과],학생3[국,영,수,과]]
>x_data=[[100,92,95,72],[92,88,80,66],[70,59,68,77]]
>y_data=[[1],[1],[0]]  #합격(1) 합격 불합격(0) 
>```

<br>

## 2.Hypothesis(판별식)을 설정한다.

<br>

>hypothesis는 데이터를 구분 짓는 직선이였죠~ 우리의 예측 값이기도 하구요.
><br>
>여기서 데이터 속성이 하나일 때랑 차이가 있는데, 
><br>
>속성이 하나보다 많으면 rank(차원)이 2차 이상이여서 그냥 X*W+b 하시면 안 됩니다.

<br>
<br>

다수 일때는 행렬 곱을 이용하셔야 합니다!
<br>
(그 때문에 W,b의 shape도 잘 생각을 하셔야 합니다.)
<br>
<br>

>Tip을 주면 x_data의 shape에 W_data의 shape을 행렬 곱해서 y_data의 shape이 나와야합니다.
><br>
>b의 shape는 y의 shape와 같습니다.
><br>
>EX) x.shape [a,b] w.shape이 [b,c]이면  y.shape [a,c]  

<br>
<br>

그럼 hypothesis를 정의합니다~
<br>
<br>

>```python
>hypothesis=tf.matmul(X,W)+b
>```
><br>
>tf.matmul은 행렬곱을 시켜준다.

<br>

## 3.Cost함수를 설정한다.

<br>

>cost 함수는 우리가 hypothesis로 설정한 직선으로 구한 예측 값과 실제 값이 얼마나 다른가를 나타내는 함수이였습니다.

<br>
<br>
다행히도 cost함수는 똑같습니다!
<br>
<br>

>```python
>cost=tf.reduce_mean(tf.square(hypothesis-Y))
>```

<br>
<br>

## 4.cost함수를 최소화시키는 W,b를 구한다.

<br>

>컴퓨터는 데이터를 훈련시키면서 cost 함수(우리가 예측한 값과 실제 값의 차이)를 작게 하는 W와 b 값을 구합니다.

<br>
<br>
이 부분도 역시 같습니다~!
<br>
<br>

>```python
>optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.00001)
>train=optimizer.minimize(cost)
>```

<br>
<br>
다음 장에서 직접 tensorflow를 이용해서 직접 훈련도 시켜보고 예측 값도 구해보겠습니다.

